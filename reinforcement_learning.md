---
tags: python,numpy,neural-network,reinforcement learning
mathjax: true
---
# An Introduction to Reinforcement Learning

Reinforcement Learning is a learning method that optimizes the execution of actions in an environment based on its state information to maximize the reward returned from the environment usually without given prior knowledge.

====> TODO IMAGE agent, action, environment, state, reward

Actions are executed by a reinforcement learning agent exploiting the already learned experience but also exploring the environment searching for possibilities to maximize the collected reward.
The right balance between exploitation and exploration is a basic problem in reinforcement learning, meaning to leverage valuable knowledge and in parallel allow to discover unknown territory, which may lead to improved behavior in the sense of larger reward to be collected.

## The Markov Decision Process (MDP)

- [Markov Decision Process (Wiki EN)](https://en.wikipedia.org/wiki/Markov_decision_process)

### The Markov property

- all information about the environment is available through its observable state
  - position, velocity, acceleration, etc.
- there is no hidden influence from the past to the future behavior of the environment, meaning the environment does not have a memory state, which is excluded from the observable state
- the future behavior of the environment only depends on its current state and the actions executed in it
- a process with these properties is called a Markov Decision Process

## Reinforcement Learning Algorithms

- Monte Carlo RL Methods (MC)

- Temporal Difference Learning (TD)
  - The Bellman Equation
  - The Q Learning Algorithm
  - The SARSA Algorithm
  - The Expected SARSA Algorithm
  - The Dyna-Q Algorithm

- Deep Q Network (DQN) Architectures

- Policy Gradient Methods

- Monte Carlo Tree Search Algorithms
  - The Alpha Zero Algorithm

- Cross Entropy RL Methods
  - learns policy by filtering out low-reward episodes trajectory data and favor high-reward episodes trajectory data

## Vocabulary

- environment
  - deterministic state change
  - stochastic state change
  - deterministic reward
  - stochastic reward
  - environment dynamics
  - branching factor
    - number of possible actions in a given state

- agent
  - state
    - observation of the environment, sometimes also called node e.g. in MCTS
  - reward
    - a reward or penalty signal returned by the environment associated with the state and action chosen by the agent
    - reward is usually a sparse signal and may be delayed relative to the triggering agent action(s)
  - action
    - an action executed in the environment resulting in a state change, sometimes in combination with a reward signal
  - model-free algorithms
  - model-based algorithms
  - experience replay buffer
    - buffer of experiance data used for learning based on batch sampling
  
- state value function V(s)
  - state-action value function Q(s, a)
  - advantage function A(s, a)

- policy
  - maps a state to an action or a vector of action probabilities: $$\pi(s) = a$$
  - deterministic policy
  - stochastic policy
  - greedy policy
    - a deterministic exploitation policy without any portion of random exploration
  - epsilon-greedy policy
    - a mostly deterministic exploitation policy with a usually small portion of random exploration
  - on-policy
    - if the data used for learning is generated following the current policy
  - off-policy
    - if the data used for learning is generated by a policy which differs from the current behavior policy
      - if the data is generated using a slightly different policy, e.g. Q learning uses a greedy policy in the next state to generate learning data while following an epsilon-greedy behavior policy
      - if the data is put into an experiance buffer, which is then used by the learning process for sampling random batches
      - if two networks are used to disentangle behavior and target policy
  - policy evaluation
    - update of state values or state-action values given a fixed policy, usually until all values converge and do not change anymore
  - policy improvement
    - policy update based on state values or state-action values
  - (general) policy iteration
    - process of interaction between policy evaluation and policy improvement
  - optimistic initial values
    - enforces environment exploration until all values have converged to their smaller optimal values keeping only the policy based portion of exploration
    - in contrast pessimistic initial values suppress exploration and the only source of exploration may be the random action selection based on the policy
  - how to learn a policy
    - learn policy directly
    - learn state action values and infer policy
    - learn environment model and infer policy by planning

- objective function
  - a reward function which can be optimized to maximize the expected discounted future reward
  - discount factor
    - determines the effect of future reward values depending on their distance from a state
    - a parameter usually chosen between 0.95 and 1.00
    - the closer to 1, the higher the impact of distant reward values

- replay buffer
  - increases sample efficiency by using buffer entries more than once during neural network training
  - using sampled batches of training data from the replay buffer results in more stable training than directly using potentially correlated sequential observations
  - prioritized replay for faster learning

- model
  - usually a representation of the environment which enables planning

## Miscellaneous Topics

- [An Introduction to DeepReinforcement Learning](https://arxiv.org/pdf/1811.12560.pdf)
- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)
- [Dueling Network Architectures for Deep Reinforcement Learning](http://proceedings.mlr.press/v48/wangf16.pdf)
- [Bellman equation (Wiki EN)](https://en.wikipedia.org/wiki/Bellman_equation)
- [Temporal Difference Learning (Wiki DE)](https://de.wikipedia.org/wiki/Temporal_Difference_Learning)
- [A Beginner's Guide to Deep Reinforcement Learning (A.I. Wiki)](https://pathmind.com/wiki/deep-reinforcement-learning)
- [Berkeley CS 294-112: Deep Reinforcement Learning](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_7_advanced_q_learning.pdf)
- actor critic
- async advantage actor critic (A3C)
- Learning values across many orders of magnitude "adaptive normalization"
- A Distributional Perspective on Reinforcement Learning
- Recurrent DQN


